{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive samples with GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_data = np.array([w[:,:-1,:].flatten() for w in wake_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.99, whiten=True)\n",
    "data = pca.fit_transform(gmm_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(1, 20)\n",
    "models = [GMM(n, covariance_type='full', random_state=0)\n",
    "          for n in n_components]\n",
    "aics = [model.fit(data).aic(data) for model in models]\n",
    "plt.plot(n_components, aics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(5, covariance_type='full', random_state=0)\n",
    "gmm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = gmm.sample(100)\n",
    "data_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakes_new = pca.inverse_transform(data_new[0])\n",
    "wakes_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakes_new = wakes_new.reshape(100,4, 28, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.make_12_heatmaps(wakes_new[:12,3,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 1680\n",
    "IMAGE_WIDTH = 60\n",
    "IMAGE_HEIGHT = 28\n",
    "code_size = 200\n",
    "num_epochs = 10\n",
    "\n",
    "lr = 0.002\n",
    "optimizer_cls = optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, code_size):\n",
    "        super().__init__()\n",
    "        self.code_size = code_size\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_cnn_1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.enc_cnn_2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.enc_linear_1 = nn.Linear(4 * 12 * 20, 50)\n",
    "        self.enc_linear_2 = nn.Linear(50, self.code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_linear_1 = nn.Linear(self.code_size, 160)\n",
    "        self.dec_linear_2 = nn.Linear(160, IMAGE_SIZE)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        code = self.encode(images)\n",
    "        out = self.decode(code)\n",
    "        return out, code\n",
    "    \n",
    "    def encode(self, images):\n",
    "        code = self.enc_cnn_1(images)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        \n",
    "        code = self.enc_cnn_2(code)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        \n",
    "        code = code.view([images.size(0), -1])\n",
    "        code = F.selu(self.enc_linear_1(code))\n",
    "        code = self.enc_linear_2(code)\n",
    "        return code\n",
    "    \n",
    "    \n",
    "    def decode(self, code):\n",
    "        \n",
    "        out = F.selu(self.dec_linear_1(code))\n",
    "        out = torch.sigmoid(self.dec_linear_2(out))\n",
    "\n",
    "        out = out.view([code.size(0), 1, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "loss_fn = nn.MCE()\n",
    "optimizer = optimizer_cls(autoencoder.parameters(), lr=lr)\n",
    "autoencoder  = autoencoder.double()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    \n",
    "    for i, images in enumerate(train_loader):    # Ignore image labels\n",
    "        out = autoencoder(images.unsqueeze(1))\n",
    "        loss = loss_fn(out, images.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Loss = %.3f\" % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early CNN Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(50*434*4, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 50*434*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = CNN1().to('cpu')\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1\n",
    "data_in = torch.Tensor(data_in)\n",
    "data_out = torch.Tensor(data_out)\n",
    "dataset = TensorDataset(data_in, data_out)\n",
    "#train_dataset, val_dataset = random_split(dataset, [int(np.ceil(len(data_in)*0.99)), int(np.floor(len(data_in)*0.01))])\n",
    "train_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in.view(1, 1, 1750, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data_in.view(1, 1, 1750, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for epoch in range(1):\n",
    "    for i, (batch_x, batch_y) in enumerate(train_data_loader):\n",
    "        # Put data in the correct device\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device).long()\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(batch_x.view(batch_size,1,1750, 28))\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        #if count % 5000 == 0:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    v_loss = 0\n",
    "\n",
    "#     for val_x, val_y in val_data_loader:\n",
    "\n",
    "#         # Put data in the correct device\n",
    "#         val_x = val_x.to(device)\n",
    "#         val_y = val_y.to(device).long()\n",
    "#         # Forward pass only to get logits/output\n",
    "#         with torch.no_grad():\n",
    "#             output = model(val_x.view(batch_size,1, seq_len))\n",
    "\n",
    "#         # Get predictions from the maximum value\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         val_bloss = criterion(output, val_y)\n",
    "#         v_loss += val_bloss*batch_size\n",
    "\n",
    "#         # Total correct predictions\n",
    "#         total += batch_size\n",
    "#         correct += (predicted == val_y).sum()\n",
    "#    accuracy = 100 * correct / total\n",
    "#    v_loss = v_loss/total\n",
    "\n",
    "    # Print Loss\n",
    "#    print('Epoch: {}. Loss: {}. ValLoss: {}. Accuracy: {} %'.format(epoch, loss.item(), v_loss, accuracy))\n",
    "    print('Epoch: {}. Loss: {}. %'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
