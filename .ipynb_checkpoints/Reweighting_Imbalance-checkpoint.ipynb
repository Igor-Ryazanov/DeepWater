{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import Scripts.cifar_resnet as rn\n",
    "import Scripts.meta_resnet as mrn\n",
    "import Scripts.wake_processing as wp\n",
    "import importlib\n",
    "import Scripts.autoencoder as ae\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Scripts.smoothing as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Scripts.wake_processing' from 'E:\\\\Studies\\\\Deep Water\\\\DeepWater\\\\Scripts\\\\wake_processing.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mrn)\n",
    "importlib.reload(wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_wakes = np.load(\"Data\\\\Wake data\\\\confirmed_wakes.npy\")\n",
    "generated_wakes = np.load(\"Data\\\\Wake data\\\\generated_wakes.npy\")\n",
    "not_wakes = np.load(\"Data\\\\Wake data\\\\negative_day_samples.npy\")\n",
    "clean_negative_samples = np.load(\"Data\\\\Wake data\\\\clean_negative_samples.npy\")\n",
    "real_ws = real_wakes[:,0,-1,0]\n",
    "real_wakes = real_wakes[:,:,:-1,:]\n",
    "not_wakes = not_wakes[:,:,:-1,:]\n",
    "train_real, test_real = train_test_split(real_wakes, test_size=0.2, random_state=4)\n",
    "train_gen, test_gen = train_test_split(generated_wakes, test_size=0.08, random_state=4)\n",
    "train_neg, test_neg = train_test_split(not_wakes, test_size = 0.06, random_state = 4)\n",
    "test_pos = np.append(test_real, test_gen, axis = 0)\n",
    "train_pos = np.append(train_real, train_gen, axis = 0)\n",
    "test_x = np.append(test_pos, test_neg, axis = 0)\n",
    "train_x = np.append(train_pos, train_neg, axis = 0)\n",
    "test_pos_y = np.concatenate((np.ones(test_pos.shape[0]).reshape(-1,1), np.zeros(test_pos.shape[0]).reshape(-1,1)), axis = 1)\n",
    "test_neg_y = np.concatenate((np.zeros(test_neg.shape[0]).reshape(-1,1), np.ones(test_neg.shape[0]).reshape(-1,1)), axis = 1)\n",
    "train_pos_y = np.concatenate((np.ones(train_pos.shape[0]).reshape(-1,1), np.zeros(train_pos.shape[0]).reshape(-1,1)), axis = 1)\n",
    "train_neg_y = np.concatenate((np.zeros(train_neg.shape[0]).reshape(-1,1), np.ones(train_neg.shape[0]).reshape(-1,1)), axis = 1)\n",
    "test_y = np.append(test_pos_y, test_neg_y, axis = 0)\n",
    "train_y = np.append(train_pos_y, train_neg_y, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "test_dataset = TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rn(model, train_data, test_data, num_epochs=5, batch_size=5, learning_rate=1e-3):\n",
    "    model = model.to(device)\n",
    "    torch.manual_seed(0)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0, drop_last=True)\n",
    "    X_val, y_val = test_data.tensors[0].to(device), test_data.tensors[1].to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    train_accs= []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        batch_accs = []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_pred_label = 1 - torch.argmax(pred, dim = 1).cpu().detach().numpy()\n",
    "            train_true_label = 1 - torch.argmax(y, dim = 1).cpu().detach().numpy()\n",
    "            batch_acc = accuracy_score(train_true_label, train_pred_label)\n",
    "            batch_accs.append(batch_acc)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "        val_pred = model(X_val)\n",
    "        val_pred_label = 1 - torch.argmax(val_pred, dim = 1).cpu().detach().numpy()\n",
    "        val_true_label = 1 - torch.argmax(y_val, dim = 1).cpu().detach().numpy()\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        train_loss = sum(total_loss)/len(total_loss)\n",
    "        train_acc = sum(batch_accs)/len(batch_accs)\n",
    "        val_acc = accuracy_score(val_true_label, val_pred_label)\n",
    "        print('Epoch:{}, Loss:{:.4f}, Val Loss:{:.4f}, Train Acc:{:4f}, Val Acc:{:.4f}'.format(epoch+1, train_loss,\n",
    "                                                                                               val_loss, train_acc, val_acc))\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        train_accs.append(train_acc)\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_losses, label = 'Train loss')\n",
    "    plt.plot(val_losses, label = 'Validation loss')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Cross-entropy loss')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_accs, label = 'Train accuracy')\n",
    "    plt.plot(val_accs, label = 'Validation accuracy')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tiny clean validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 4, 28, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_real.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_real = []\n",
    "for i in [0, 3, 5, 7, 9]:\n",
    "    tiny_real.append(train_real[i,:,:,:])\n",
    "tiny_real = np.array(tiny_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4, 29, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_negative_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_negative = []\n",
    "for i in [15, 17, 19, 21, 26]:\n",
    "    tiny_negative.append(clean_negative_samples[i,:,:-1,:])\n",
    "tiny_negative = np.array(tiny_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_X = np.append(tiny_real, tiny_negative, axis = 0)\n",
    "tiny_pos_y = np.concatenate((np.ones(5).reshape(-1,1), np.zeros(5).reshape(-1,1)), axis = 1)\n",
    "tiny_neg_y = np.concatenate((np.zeros(5).reshape(-1,1), np.ones(5).reshape(-1,1)), axis = 1)\n",
    "tiny_y = np.append(tiny_pos_y, tiny_neg_y, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_X = torch.tensor(tiny_X).float().to(device)\n",
    "tiny_y = torch.tensor(tiny_y).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 28, 60])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_resnet = mrn.ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss  = nn.BCELoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.5, 0.6])\n",
    "b = torch.tensor([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931, 0.7136])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_loss(a, b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net = mrn.ResNet18()\n",
    "learning_rate = 1e-3\n",
    "net = net.to(device)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss(reduction = 'none')\n",
    "criterion_mean = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "meta_losses_clean = []\n",
    "net_losses = []\n",
    "plot_step = 100\n",
    "\n",
    "smoothing_alpha = 0.9\n",
    "\n",
    "meta_l = 0\n",
    "net_l = 0\n",
    "accuracy_log = []\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=0, drop_last=True)\n",
    "X_val, y_val = test_dataset.tensors[0].to(device), test_dataset.tensors[1].to(device)\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = []\n",
    "    batch_accs = []\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "\n",
    "        meta_net = mrn.ResNet18().to(device)\n",
    "        meta_net.load_state_dict(net.state_dict())\n",
    "        # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n",
    "\n",
    "        y_f_hat  = meta_net(X)\n",
    "        cost = criterion(y_f_hat,y)\n",
    "        \n",
    "        eps = torch.zeros(cost.size()).to(device)\n",
    "        eps.requires_grad = True\n",
    "        l_f_meta = torch.sum(cost * eps)\n",
    "        meta_net.zero_grad()\n",
    "\n",
    "\n",
    "         # Line 6 perform a parameter update\n",
    "        grads = torch.autograd.grad(l_f_meta, (meta_net.parameters()), create_graph=True)\n",
    "        meta_net.update_params(learning_rate, source_params=grads)\n",
    "        \n",
    "        # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n",
    "        y_g_hat = meta_net(tiny_X)\n",
    "\n",
    "        l_g_meta = criterion_mean(y_g_hat,tiny_y)\n",
    "\n",
    "        grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n",
    "        \n",
    "        # Line 11 computing and normalizing the weights\n",
    "        \n",
    "        w_tilde = torch.clamp(-grad_eps,min=0)\n",
    "        norm_c = torch.sum(w_tilde)\n",
    "\n",
    "        if norm_c != 0:\n",
    "            w = w_tilde / norm_c\n",
    "        else:\n",
    "            w = w_tilde\n",
    "        \n",
    "        # Lines 12 - 14 computing for the loss with the computed weights\n",
    "        # and then perform a parameter update\n",
    "        y_f_hat = net(X)\n",
    "        cost = criterion(y_f_hat,y)\n",
    "        l_f = torch.sum(cost * w)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        l_f.backward()\n",
    "        opt.step()\n",
    "        meta_l = smoothing_alpha *meta_l + (1 - smoothing_alpha)* l_g_meta.item()\n",
    "        meta_losses_clean.append(meta_l/(1 - smoothing_alpha**(i+1)))\n",
    "\n",
    "        net_l = smoothing_alpha *net_l + (1 - smoothing_alpha)* l_f.item()\n",
    "        net_losses.append(net_l/(1 - smoothing_alpha**(i+1)))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weights(net, train_data, test_data, num_epochs=5, batch_size=5,  learning_rate=1e-3):\n",
    "    net = net.to(device)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss(reduction = 'none')\n",
    "    criterion_mean = nn.BCELoss(reduction = 'mean')\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0, drop_last=True)\n",
    "    X_val, y_val = test_data.tensors[0].to(device), test_data.tensors[1].to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    train_accs= []\n",
    "    \n",
    "    \n",
    "#     meta_losses_clean = []\n",
    "#     net_losses = []\n",
    "#     plot_step = 100\n",
    "\n",
    "#     smoothing_alpha = 0.9\n",
    "    \n",
    "#     meta_l = 0\n",
    "#     net_l = 0\n",
    "#     accuracy_log = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        batch_accs = []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            \n",
    "            meta_net = mrn.ResNet18().to(device)\n",
    "            meta_net.load_state_dict(net.state_dict())\n",
    "            # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n",
    "            \n",
    "            y_f_hat  = meta_net(X)\n",
    "            cost = criterion(y_f_hat,y)\n",
    "            eps = torch.zeros(cost.size()).to(device)\n",
    "            eps.requires_grad = True\n",
    "            l_f_meta = torch.sum(cost * eps)\n",
    "            meta_net.zero_grad()\n",
    "            \n",
    "            \n",
    "             # Line 6 perform a parameter update\n",
    "            grads = torch.autograd.grad(l_f_meta, (meta_net.parameters()), create_graph=True)\n",
    "            meta_net.update_params(learning_rate, source_params=grads)\n",
    "\n",
    "            # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n",
    "            y_g_hat = meta_net(tiny_X)\n",
    "\n",
    "            l_g_meta = criterion_mean(y_g_hat,tiny_y)\n",
    "\n",
    "            grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n",
    "            \n",
    "            # Line 11 computing and normalizing the weights\n",
    "        \n",
    "            w_tilde = torch.clamp(-grad_eps,min=0)\n",
    "            norm_c = torch.sum(w_tilde)\n",
    "\n",
    "            if norm_c != 0:\n",
    "                w = w_tilde / norm_c\n",
    "            else:\n",
    "                w = w_tilde\n",
    "\n",
    "            # Lines 12 - 14 computing for the loss with the computed weights\n",
    "            # and then perform a parameter update\n",
    "            y_f_hat = net(X)\n",
    "            cost = criterion(y_f_hat,y)\n",
    "            l_f = torch.sum(cost * w)\n",
    "\n",
    "            train_pred_label = 1 - torch.argmax(y_f_hat, dim = 1).cpu().detach().numpy()\n",
    "            train_true_label = 1 - torch.argmax(y, dim = 1).cpu().detach().numpy()\n",
    "            batch_acc = accuracy_score(train_true_label, train_pred_label)\n",
    "            batch_accs.append(batch_acc)\n",
    "            total_loss.append(torch.mean(cost).cpu().detach().numpy())\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            l_f.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "        #Checking accuracy/loss\n",
    "        net.eval()\n",
    "        \n",
    "        train_loss = sum(total_loss)/len(total_loss)\n",
    "        train_acc = sum(batch_accs)/len(batch_accs)\n",
    "        val_pred = net(X_val)\n",
    "        val_pred_label = 1 - torch.argmax(val_pred, dim = 1).cpu().detach().numpy()\n",
    "        val_true_label = 1 - torch.argmax(y_val, dim = 1).cpu().detach().numpy()\n",
    "        val_loss = criterion_mean(val_pred, y_val).item()\n",
    "        val_acc = accuracy_score(val_true_label, val_pred_label)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        train_accs.append(train_acc)\n",
    "        print('Epoch:{}, Loss:{:.4f}, Val Loss:{:.4f}, Train Acc:{:4f}, Val Acc:{:.4f}'.format(epoch+1, train_loss,\n",
    "                                                                                           val_loss, train_acc, val_acc))\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:0.3694, Val Loss:0.4020, Train Acc:0.851449, Val Acc:0.8305\n",
      "Epoch:2, Loss:0.2590, Val Loss:0.3234, Train Acc:0.907971, Val Acc:0.8644\n",
      "Epoch:3, Loss:0.1972, Val Loss:0.9538, Train Acc:0.928986, Val Acc:0.7203\n",
      "Epoch:4, Loss:0.1873, Val Loss:0.4110, Train Acc:0.936232, Val Acc:0.8390\n",
      "Epoch:5, Loss:0.1932, Val Loss:0.3503, Train Acc:0.934058, Val Acc:0.8729\n",
      "Epoch:6, Loss:0.1547, Val Loss:0.3882, Train Acc:0.948551, Val Acc:0.8729\n",
      "Epoch:7, Loss:0.1477, Val Loss:0.2710, Train Acc:0.943478, Val Acc:0.9237\n",
      "Epoch:8, Loss:0.1378, Val Loss:0.4161, Train Acc:0.951449, Val Acc:0.8729\n",
      "Epoch:9, Loss:0.1378, Val Loss:0.5889, Train Acc:0.955797, Val Acc:0.8475\n",
      "Epoch:10, Loss:0.1374, Val Loss:0.3143, Train Acc:0.955072, Val Acc:0.9153\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-861ef02a9f6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresnet_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmrn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResNet18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "resnet_1 = mrn.ResNet18()\n",
    "train_losses, val_losses, train_accs, val_accs = train_weights(resnet_1, train_dataset, test_dataset, num_epochs = 10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
