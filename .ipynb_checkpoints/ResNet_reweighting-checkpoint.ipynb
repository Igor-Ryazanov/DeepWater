{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Scripts.cifar_resnet as rn\n",
    "import Scripts.wake_processing as wp\n",
    "import importlib\n",
    "import Scripts.autoencoder as ae\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Scripts.smoothing as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Scripts.wake_processing' from 'E:\\\\Studies\\\\Deep Water\\\\DeepWater\\\\Scripts\\\\wake_processing.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(rn)\n",
    "importlib.reload(wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_wakes = np.load(\"Data\\\\Wake data\\\\confirmed_wakes.npy\")\n",
    "generated_wakes = np.load(\"Data\\\\Wake data\\\\generated_wakes.npy\")\n",
    "not_wakes = np.load(\"Data\\\\Wake data\\\\negative_day_samples.npy\")\n",
    "clean_negative_samples = np.load(\"Data\\\\Wake data\\\\clean_negative_samples.npy\")\n",
    "real_ws = real_wakes[:,0,-1,0]\n",
    "real_wakes = real_wakes[:,:,:-1,:]\n",
    "not_wakes = not_wakes[:,:,:-1,:]\n",
    "train_real, test_real = train_test_split(real_wakes, test_size=0.2, random_state=4)\n",
    "train_gen, test_gen = train_test_split(generated_wakes, test_size=0.08, random_state=4)\n",
    "train_neg, test_neg = train_test_split(not_wakes, test_size = 0.06, random_state = 4)\n",
    "test_pos = np.append(test_real, test_gen, axis = 0)\n",
    "train_pos = np.append(train_real, train_gen, axis = 0)\n",
    "test_x = np.append(test_pos, test_neg, axis = 0)\n",
    "train_x = np.append(train_pos, train_neg, axis = 0)\n",
    "test_pos_y = np.concatenate((np.ones(test_pos.shape[0]).reshape(-1,1), np.zeros(test_pos.shape[0]).reshape(-1,1)), axis = 1)\n",
    "test_neg_y = np.concatenate((np.zeros(test_neg.shape[0]).reshape(-1,1), np.ones(test_neg.shape[0]).reshape(-1,1)), axis = 1)\n",
    "train_pos_y = np.concatenate((np.ones(train_pos.shape[0]).reshape(-1,1), np.zeros(train_pos.shape[0]).reshape(-1,1)), axis = 1)\n",
    "train_neg_y = np.concatenate((np.zeros(train_neg.shape[0]).reshape(-1,1), np.ones(train_neg.shape[0]).reshape(-1,1)), axis = 1)\n",
    "test_y = np.append(test_pos_y, test_neg_y, axis = 0)\n",
    "train_y = np.append(train_pos_y, train_neg_y, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "test_dataset = TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 4, 28, 60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rn(model, train_data, test_data, num_epochs=5, batch_size=5, learning_rate=1e-3):\n",
    "    model = model.to(device)\n",
    "    torch.manual_seed(0)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0, drop_last=True)\n",
    "    X_val, y_val = test_data.tensors[0].to(device), test_data.tensors[1].to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    train_accs= []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        batch_accs = []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_pred_label = 1 - torch.argmax(pred, dim = 1).cpu().detach().numpy()\n",
    "            train_true_label = 1 - torch.argmax(y, dim = 1).cpu().detach().numpy()\n",
    "            batch_acc = accuracy_score(train_true_label, train_pred_label)\n",
    "            batch_accs.append(batch_acc)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "        val_pred = model(X_val)\n",
    "        val_pred_label = 1 - torch.argmax(val_pred, dim = 1).cpu().detach().numpy()\n",
    "        val_true_label = 1 - torch.argmax(y_val, dim = 1).cpu().detach().numpy()\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        train_loss = sum(total_loss)/len(total_loss)\n",
    "        train_acc = sum(batch_accs)/len(batch_accs)\n",
    "        val_acc = accuracy_score(val_true_label, val_pred_label)\n",
    "        print('Epoch:{}, Loss:{:.4f}, Val Loss:{:.4f}, Train Acc:{:4f}, Val Acc:{:.4f}'.format(epoch+1, train_loss,\n",
    "                                                                                               val_loss, train_acc, val_acc))\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        train_accs.append(train_acc)\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:0.2876, Val Loss:0.2402, Train Acc:0.886957, Val Acc:0.9068\n",
      "Epoch:2, Loss:0.1623, Val Loss:0.2849, Train Acc:0.944203, Val Acc:0.8898\n",
      "Epoch:3, Loss:0.1145, Val Loss:0.1827, Train Acc:0.957971, Val Acc:0.9407\n",
      "Epoch:4, Loss:0.0625, Val Loss:0.3311, Train Acc:0.979710, Val Acc:0.8729\n",
      "Epoch:5, Loss:0.0241, Val Loss:0.2603, Train Acc:0.996377, Val Acc:0.9492\n",
      "Epoch:6, Loss:0.0538, Val Loss:0.3971, Train Acc:0.979710, Val Acc:0.8898\n",
      "Epoch:7, Loss:0.0408, Val Loss:0.3419, Train Acc:0.984783, Val Acc:0.9237\n",
      "Epoch:8, Loss:0.0215, Val Loss:0.4215, Train Acc:0.992029, Val Acc:0.8983\n",
      "Epoch:9, Loss:0.0180, Val Loss:0.5793, Train Acc:0.992754, Val Acc:0.8390\n",
      "Epoch:10, Loss:0.0126, Val Loss:0.5812, Train Acc:0.995652, Val Acc:0.8983\n",
      "Epoch:11, Loss:0.0011, Val Loss:0.5665, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:12, Loss:0.0011, Val Loss:0.5346, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:13, Loss:0.0006, Val Loss:0.6125, Train Acc:1.000000, Val Acc:0.8898\n",
      "Epoch:14, Loss:0.0003, Val Loss:0.6327, Train Acc:1.000000, Val Acc:0.8898\n",
      "Epoch:15, Loss:0.0002, Val Loss:0.6752, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:16, Loss:0.0002, Val Loss:0.6725, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:17, Loss:0.0002, Val Loss:0.6927, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:18, Loss:0.0001, Val Loss:0.7105, Train Acc:1.000000, Val Acc:0.8983\n",
      "Epoch:19, Loss:0.0001, Val Loss:0.6956, Train Acc:1.000000, Val Acc:0.8983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6b2b6a6552a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResNet18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_rn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-1cfa64629da6>\u001b[0m in \u001b[0;36mtrain_rn\u001b[1;34m(model, train_data, test_data, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mtrain_pred_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet = None\n",
    "resnet = rn.ResNet18()\n",
    "train_losses, val_losses, train_accs, val_accs = train_rn(resnet, train_dataset, test_dataset, num_epochs = 50, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-07583e63e4ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Validation loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'upper left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_losses, label = 'Train loss')\n",
    "plt.plot(val_losses, label = 'Validation loss')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_accs, label = 'Train accuracy')\n",
    "plt.plot(val_accs, label = 'Validation accuracy')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tiny clean validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 4, 28, 60)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_real = []\n",
    "for i in [0, 3, 5, 7, 9]:\n",
    "    tiny_real.append(train_real[i,:,:,:])\n",
    "tiny_real = np.array(tiny_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4, 29, 60)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_negative_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_negative = []\n",
    "for i in [15, 17, 19, 21, 26]:\n",
    "    tiny_negative.append(clean_negative_samples[i,:,:-1,:])\n",
    "tiny_negative = np.array(tiny_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_X = np.append(tiny_real, tiny_negative, axis = 0)\n",
    "tiny_y = np.append(np.ones(5).reshape(-1,1), np.zeros(5).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = model.to(device)\n",
    "    torch.manual_seed(0)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0, drop_last=True)\n",
    "    X_val, y_val = test_data.tensors[0].to(device), test_data.tensors[1].to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    train_accs= []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        batch_accs = []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_pred_label = 1 - torch.argmax(pred, dim = 1).cpu().detach().numpy()\n",
    "            train_true_label = 1 - torch.argmax(y, dim = 1).cpu().detach().numpy()\n",
    "            batch_acc = accuracy_score(train_true_label, train_pred_label)\n",
    "            batch_accs.append(batch_acc)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "        val_pred = model(X_val)\n",
    "        val_pred_label = 1 - torch.argmax(val_pred, dim = 1).cpu().detach().numpy()\n",
    "        val_true_label = 1 - torch.argmax(y_val, dim = 1).cpu().detach().numpy()\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        train_loss = sum(total_loss)/len(total_loss)\n",
    "        train_acc = sum(batch_accs)/len(batch_accs)\n",
    "        val_acc = accuracy_score(val_true_label, val_pred_label)\n",
    "        print('Epoch:{}, Loss:{:.4f}, Val Loss:{:.4f}, Train Acc:{:4f}, Val Acc:{:.4f}'.format(epoch+1, train_loss,\n",
    "                                                                                               val_loss, train_acc, val_acc))\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        train_accs.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lre(net, learning_rate=1e-3):\n",
    "    net = net.to(device)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    meta_losses_clean = []\n",
    "    net_losses = []\n",
    "    plot_step = 100\n",
    "\n",
    "    smoothing_alpha = 0.9\n",
    "    \n",
    "    meta_l = 0\n",
    "    net_l = 0\n",
    "    accuracy_log = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        batch_accs = []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_pred_label = 1 - torch.argmax(pred, dim = 1).cpu().detach().numpy()\n",
    "            train_true_label = 1 - torch.argmax(y, dim = 1).cpu().detach().numpy()\n",
    "            batch_acc = accuracy_score(train_true_label, train_pred_label)\n",
    "            batch_accs.append(batch_acc)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in tqdm(range(hyperparameters['num_iterations'])):\n",
    "        \n",
    "        # Line 2 get batch of data\n",
    "        image, labels = next(iter(data_loader))\n",
    "        # since validation data is small I just fixed them instead of building an iterator\n",
    "        # initialize a dummy network for the meta learning of the weights\n",
    "        meta_net = LeNet(n_out=1)\n",
    "        meta_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            meta_net.cuda()\n",
    "\n",
    "        image = to_var(image, requires_grad=False)\n",
    "        labels = to_var(labels, requires_grad=False)\n",
    "\n",
    "        # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n",
    "        y_f_hat  = meta_net(image)\n",
    "        cost = F.binary_cross_entropy_with_logits(y_f_hat,labels, reduce=False)\n",
    "        eps = to_var(torch.zeros(cost.size()))\n",
    "        l_f_meta = torch.sum(cost * eps)\n",
    "\n",
    "        meta_net.zero_grad()\n",
    "        \n",
    "        # Line 6 perform a parameter update\n",
    "        grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n",
    "        meta_net.update_params(hyperparameters['lr'], source_params=grads)\n",
    "        \n",
    "        # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n",
    "        y_g_hat = meta_net(val_data)\n",
    "\n",
    "        l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat,val_labels)\n",
    "\n",
    "        grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n",
    "        \n",
    "        # Line 11 computing and normalizing the weights\n",
    "        w_tilde = torch.clamp(-grad_eps,min=0)\n",
    "        norm_c = torch.sum(w_tilde)\n",
    "\n",
    "        if norm_c != 0:\n",
    "            w = w_tilde / norm_c\n",
    "        else:\n",
    "            w = w_tilde\n",
    "\n",
    "        # Lines 12 - 14 computing for the loss with the computed weights\n",
    "        # and then perform a parameter update\n",
    "        y_f_hat = net(image)\n",
    "        cost = F.binary_cross_entropy_with_logits(y_f_hat, labels, reduce=False)\n",
    "        l_f = torch.sum(cost * w)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        l_f.backward()\n",
    "        opt.step()\n",
    "\n",
    "        meta_l = smoothing_alpha *meta_l + (1 - smoothing_alpha)* l_g_meta.item()\n",
    "        meta_losses_clean.append(meta_l/(1 - smoothing_alpha**(i+1)))\n",
    "\n",
    "        net_l = smoothing_alpha *net_l + (1 - smoothing_alpha)* l_f.item()\n",
    "        net_losses.append(net_l/(1 - smoothing_alpha**(i+1)))\n",
    "\n",
    "        if i % plot_step == 0:\n",
    "            net.eval()\n",
    "\n",
    "            acc = []\n",
    "            for itr,(test_img, test_label) in enumerate(test_loader):\n",
    "                test_img = to_var(test_img, requires_grad=False)\n",
    "                test_label = to_var(test_label, requires_grad=False)\n",
    "\n",
    "                output = net(test_img)\n",
    "                predicted = (F.sigmoid(output) > 0.5).int()\n",
    "\n",
    "                acc.append((predicted.int() == test_label.int()).float())\n",
    "\n",
    "            accuracy = torch.cat(acc,dim=0).mean()\n",
    "            accuracy_log.append(np.array([i,accuracy])[None])\n",
    "\n",
    "\n",
    "            IPython.display.clear_output()\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(13,5))\n",
    "            ax1, ax2 = axes.ravel()\n",
    "\n",
    "            ax1.plot(meta_losses_clean, label='meta_losses_clean')\n",
    "            ax1.plot(net_losses, label='net_losses')\n",
    "            ax1.set_ylabel(\"Losses\")\n",
    "            ax1.set_xlabel(\"Iteration\")\n",
    "            ax1.legend()\n",
    "\n",
    "            acc_log = np.concatenate(accuracy_log, axis=0)\n",
    "            ax2.plot(acc_log[:,0],acc_log[:,1])\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.set_xlabel('Iteration')\n",
    "            plt.show()\n",
    "            \n",
    "        # return accuracy\n",
    "    return np.mean(acc_log[-6:-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
